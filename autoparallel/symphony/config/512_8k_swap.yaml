auto_trans_ckpt: false
callbacks:
-   per_print_times: 1
    type: MFLossMonitor
context:
    ascend_config:
        parallel_speed_up_json_path: /data/x00669956/mindformers-dev/research/deepseek3/parallel_speed_up.json
    device_target: Ascend
    enable_interleave_parallel_branch: true
    jit_config:
        jit_level: O1
    max_call_depth: 10000
    max_device_memory: 54GB
    mode: 0
    save_graphs: true
    save_graphs_path: ./graph
init_start_profile: false
lr_schedule:
    learning_rate: 0.00022
    total_steps: -1
    type: ConstantWarmUpLR
    warmup_steps: 20
micro_batch_interleave_num: 1
model:
    arch:
        auto_register: deepseek3_model.DeepseekV3ForCausalLM
        type: DeepseekV3ForCausalLM
    model_config:
        auto_register: deepseek3_config.DeepseekV3Config
        batch_size: 1
        bos_token_id: 100000
        checkpoint_name_or_path: ''
        compute_dtype: bfloat16
        eos_token_id: 100001
        extend_method: None
        hidden_size: 7168
        ignore_token_id: -100
        input_sliced_sig: true
        intermediate_size: 18432
        kv_lora_rank: 512
        layernorm_compute_type: float32
        max_position_embeddings: 163840
        mtp_depth: 1
        mtp_loss_factor: 0.3
        multiple_of: 256
        n_kv_heads: 128
        num_heads: 128
        num_layers: 61
        offset: [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 0]]
        pad_token_id: 100001
        param_init_type: float32
        pp_interleave_num: 4
        q_lora_rank: 1536
        qk_nope_head_dim: 128
        qk_rope_head_dim: 64
        return_extra_loss: false
        rms_norm_eps: 1.0e-06
        rotary_dtype: float32
        router_dense_type: float32
        seq_length: 8192
        softmax_compute_type: float32
        theta: 10000.0
        type: DeepseekV3Config
        use_flash_attention: true
        use_force_expert_balance: true
        use_past: false
        v_head_dim: 128
        vocab_size: 129280
moe_config:
    aux_loss_factor: 0.05
    aux_loss_factors:
    - 0.001
    - 0.0
    - 0.0
    aux_loss_types:
    - expert
    - device
    - comm
    balance_via_topk_bias: true
    capacity_factor: 1.0
    enable_gmm_safe_tokens: false
    enable_sdrop: false
    expert_group_size: 8
    expert_model_parallel: 1
    expert_num: 256
    first_k_dense_replace: 3
    group_wise_a2a: false
    moe_intermediate_size: 2048
    moe_shared_expert_overlap: true
    n_group: 8
    norm_topk_prob: true
    num_experts_chosen: 8
    routed_scaling_factor: 2.5
    routing_policy: TopkRouterV2
    shared_expert_num: 1
    topk_bias_update_rate: 0.0001
    topk_group: 4
    use_fused_ops_topkrouter: true
    use_gating_sigmoid: false
    use_gmm: true
    z_loss_factor: 0.0
only_save_strategy: false
optimizer:
    betas:
    - 0.9
    - 0.95
    eps: 1.0e-08
    type: AdamW
output_dir: ./output
parallel:
    dataset_strategy:
    -   - 16
        - 1
    -   - 16
        - 1
    -   - 16
        - 1
    -   - 16
        - 1
    enable_alltoall: true
    enable_parallel_optimizer: true
    full_batch: false
    gradients_mean: false
    parallel_mode: 1
    parallel_optimizer_config:
        gradient_accumulation_shard: false
        parallel_optimizer_threshold: 64
    search_mode: sharding_propagation
    strategy_ckpt_save_file: ./ckpt_strategy.ckpt
    pipeline_config:
      pipeline_interleave: True
      pipeline_scheduler: 'seqpipe'
parallel_config:
    data_parallel: 16
    expert_parallel: 64
    gradient_aggregation_group: 4
    micro_batch_num: 240
    model_parallel: 4
    pipeline_stage: 8
    use_seq_parallel: true
    vocab_emb_dp: true
profile: false
profile_communication: false
profile_memory: true
profile_start_step: 8
profile_stop_step: 8
recompute_config:
    mp_comm_recompute: true
    parallel_optimizer_comm_recompute: false
    recompute: [[2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 1], [2, 2, 2, 2, 2, 2, 0, 0]]
    recompute_slice_activation: true
    # select_comm_recompute:
    #     .*\.norm: &id001
    #     -   - 0
    #         - 0
    #         - 0
    #         - 0
    #         - 0
    #         - 0
    #         - 0
    #         - 0
        # attention\.wk\.reshape: *id001
        # attention\.wq\.reshape: *id001
        # feed_forward\.w1\.reshape: *id001
        # feed_forward\.w3\.reshape: *id001
    # select_recompute:
    #     add: *id001
    #     cast_up: *id001
    #     feed_forward\.null: *id001
    #     feed_forward\.w1\.activation\.silu: *id001
    #     feed_forward\.w1\.reshape: *id001
    #     feed_forward\.w2\.reshape: *id001
resume_training: false
run_mode: train
runner_config:
    batch_size: 1
    epochs: 10
    sink_mode: true
    sink_size: 1
runner_wrapper:
    scale_sense: 1.0
    type: MFTrainOneStepCell
    use_clip_grad: true
seed: 0
src_strategy_path_or_dir: ''
train_dataset: &id002
    batch_size: 1
    construct_args_key:
    - input_ids
    - labels
    - position_ids
    - actual_seq_len
    data_loader:
        config:
            create_attention_mask: false
            create_compressed_eod_mask: true
            data_path:
            - '1'
            - /data/x00669956/code_xfc/data8k_gp/mindformers-deepseek_eod/research/deepseek3/dataset/text_document_mega1024
            eod: 100001
            eod_mask_loss: true
            eod_pad_length: 256
            num_dataset_builder_threads: 1
            pad: -1
            reset_attention_mask: true
            reset_position_ids: true
            seed: 1234
            seq_length: 8192
            shuffle: false
            split: 1, 0, 0
            type: GPTDatasetConfig
        datasets_type: GPTDataset
        sizes:
        - 30000
        - 0
        - 0
        type: BlendedMegatronDatasetDataLoader
    drop_remainder: true
    eod_reset: false
    input_columns:
    - input_ids
    - labels
    - loss_mask
    - position_ids
    - actual_seq_len
    num_parallel_workers: 1
    numa_enable: false
    output_columns:
    - input_ids
    - labels
    - position_ids
    - actual_seq_len
    prefetch_size: 1
    python_multiprocessing: false
    repeat: 1
    seed: 1234
train_dataset_task:
    dataset_config: *id002
    type: CausalLanguageModelDataset
trainer:
    model_name: deepseekV3
    type: CausalLanguageModelingTrainer
use_parallel: true
