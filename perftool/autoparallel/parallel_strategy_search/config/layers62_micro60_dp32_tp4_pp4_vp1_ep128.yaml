auto_trans_ckpt: false
auto_tune: false
autotune_per_step: 10
callbacks:
-   per_print_times: 1
    type: MFLossMonitor
-   type: ObsMonitor
context:
    ascend_config:
        parallel_speed_up_json_path: /data/x00669956/mindformers_910c/research/deepseek3/parallel_speed_up.json
    device_target: Ascend
    enable_interleave_parallel_branch: true
    jit_config:
        jit_level: O1
    max_call_depth: 10000
    max_device_memory: 54GB
    mode: 0
    save_graphs: false
    save_graphs_path: ./graph
do_eval: false
eval_callbacks:
-   type: ObsMonitor
eval_dataset: &id001
    data_loader:
        dataset_dir: ''
        shuffle: false
        type: MindDataset
    drop_remainder: false
    input_columns:
    - input_ids
    num_parallel_workers: 8
    numa_enable: false
    prefetch_size: 1
    python_multiprocessing: false
    repeat: 1
eval_dataset_task:
    dataset_config: *id001
    type: CausalLanguageModelDataset
eval_epoch_interval: 50
eval_step_interval: -1
filepath_prefix: ./autotune
init_start_profile: false
layer_decay: 0.65
layer_scale: false
load_checkpoint: ''
lr_scale_factor: 256
lr_schedule:
    learning_rate: 2.0e-05
    lr_end: 2.0e-05
    total_steps: -1
    type: CosineWithWarmUpLR
    warmup_ratio: 0.0
metric:
    type: PerplexityMetric
micro_batch_interleave_num: 1
model:
    arch:
        auto_register: deepseek3_model.DeepseekV3ForCausalLM
        type: DeepseekV3ForCausalLM
    model_config:
        auto_register: deepseek3_config.DeepseekV3Config
        batch_size: 1
        bos_token_id: 100000
        checkpoint_name_or_path: ''
        compute_dtype: bfloat16
        do_sample: false
        eos_token_id: 100001
        extend_method: None
        hidden_size: 7168
        ignore_token_id: -100
        intermediate_size: 18432
        kv_lora_rank: 512
        layernorm_compute_type: float32
        max_decode_length: 1024
        max_position_embeddings: 163840
        mtp_depth: 1
        mtp_loss_factor: 0.3
        multiple_of: 256
        n_kv_heads: 128
        num_heads: 128
        num_layers: 61
        offset:
        -   - 2
            - 1
            - 1
            - 1
        -   - 1
            - 1
            - 1
            - -2
        pad_token_id: 100001
        param_init_type: float32
        pp_interleave_num: 2
        q_lora_rank: 1536
        qk_nope_head_dim: 128
        qk_rope_head_dim: 64
        repetition_penalty: 1
        return_extra_loss: false
        rms_norm_eps: 1.0e-06
        rotary_dtype: float32
        router_dense_type: float32
        seq_length: 4096
        softmax_compute_type: float32
        theta: 10000.0
        top_k: 5
        top_p: 1
        type: DeepseekV3Config
        use_flash_attention: true
        use_force_expert_balance: true
        use_past: false
        v_head_dim: 128
        vocab_size: 129280
moe_config:
    aux_loss_factor: 0.05
    aux_loss_factors:
    - 0.001
    aux_loss_types:
    - expert
    balance_via_topk_bias: true
    capacity_factor: 1.0
    enable_gmm_safe_tokens: false
    enable_sdrop: false
    expert_group_size: 8
    expert_model_parallel: 1
    expert_num: 256
    first_k_dense_replace: 3
    group_wise_a2a: false
    moe_intermediate_size: 2048
    moe_shared_expert_overlap: true
    n_group: 8
    norm_topk_prob: true
    num_experts_chosen: 8
    routed_scaling_factor: 2.5
    routing_policy: TopkRouterV2
    shared_expert_num: 1
    topk_bias_update_rate: 0.0001
    topk_group: 4
    use_fused_ops_topkrouter: true
    use_gating_sigmoid: false
    use_gmm: true
    z_loss_factor: 0.0
only_save_strategy: false
optimizer:
    betas:
    - 0.9
    - 0.95
    eps: 1.0e-08
    learning_rate: 0
    type: AdamW
output_dir: ./output
parallel:
    enable_alltoall: true
    enable_parallel_optimizer: true
    full_batch: true
    gradients_mean: false
    parallel_mode: 1
    parallel_optimizer_config:
        gradient_accumulation_shard: false
        parallel_optimizer_threshold: 64
    pipeline_config:
        pipeline_interleave: true
        pipeline_scheduler: seqpipe
    search_mode: sharding_propagation
    strategy_ckpt_save_file: ./ckpt_strategy.ckpt
parallel_config:
    data_parallel: 32
    expert_parallel: 128
    gradient_aggregation_group: 4
    micro_batch_num: 120
    model_parallel: 4
    pipeline_stage: 4
    use_seq_parallel: true
    vocab_emb_dp: true
processor:
    return_tensors: ms
    tokenizer:
        bos_token: "<\uFF5Cbegin\u2581of\u2581sentence\uFF5C>"
        eos_token: "<\uFF5Cend\u2581of\u2581sentence\uFF5C>"
        pad_token: "<\uFF5Cend\u2581of\u2581sentence\uFF5C>"
        tokenizer_file: ./path/tokenizer.json
        type: LlamaTokenizerFast
        unk_token: <unk>
        vocab_file: ./path/tokenizer.model
    type: LlamaProcessor
profile: false
profile_communication: false
profile_memory: true
profile_start_step: 11
profile_stop_step: 12
recompute_config:
    mp_comm_recompute: true
    parallel_optimizer_comm_recompute: false
    recompute:
    -   - 0
        - 0
        - 0
        - 0
    -   - 0
        - 0
        - 0
        - 0
    recompute_slice_activation: true
    select_comm_recompute:
        .*\.norm: &id002
        -   - 0
            - 0
            - 0
            - 0
        -   - 0
            - 0
            - 0
            - 0
        attention\.wk\.reshape: *id002
        attention\.wq\.reshape: *id002
        feed_forward\.w1\.reshape: *id002
        feed_forward\.w3\.reshape: *id002
    select_recompute:
        add: *id002
        cast_up: *id002
        feed_forward\.null: *id002
        feed_forward\.w1\.activation\.silu: *id002
        feed_forward\.w1\.reshape: *id002
        feed_forward\.w2\.reshape: *id002
remote_save_url: Please input obs url on AICC platform.
resume_training: false
run_mode: train
runner_config:
    batch_size: 1
    epochs: 2
    sink_mode: true
    sink_size: 1
runner_wrapper:
    scale_sense: 1.0
    type: MFTrainOneStepCell
    use_clip_grad: true
seed: 0
src_strategy_path_or_dir: ''
train_dataset: &id003
    data_loader:
        dataset_dir: /data/x00669956/code_xfc/wiki103-4k.mindrecord
        shuffle: false
        type: MindDataset
    drop_remainder: true
    input_columns:
    - input_ids
    num_parallel_workers: 8
    numa_enable: false
    prefetch_size: 1
    python_multiprocessing: false
    repeat: 1
train_dataset_task:
    dataset_config: *id003
    type: CausalLanguageModelDataset
trainer:
    model_name: deepseekV3
    type: CausalLanguageModelingTrainer
use_parallel: true
